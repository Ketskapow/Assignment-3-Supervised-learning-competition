:::{.callout-note}
## Instruction

This is a quarto file. If you open its [source](report_template.qmd) in RStudio, you will get all the features of this interactive notebook-style document. If you want to know more about how to use this file to generate a .html document to hand in, look at [the getting started guide](https://quarto.org/docs/get-started/hello/rstudio.html).

(Delete this callout box before handing in your assignment.)
:::

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(corrr)
# additional packages here
```

```{r}
#| label: data loading
#| echo: false

testdata <- read_rds("data/test.rds")
traindata <- read_rds("data/train.rds")
```

# Data description

Before making any predictions, we want to look at the data and its structure

```{r}
#| label: data head

head(testdata)
```
We have 30 columns in this data, each row of this data likely representing a student and all their corresponding data.

We also eventually want to predict the score of a student with several attributes. We can already begin exploring the structure of some of these attributes:

```{r}
#| label: eda visualization

ggplot(traindata, aes(y = score, x = guardian)) +
  theme_minimal() +
  geom_boxplot()
```
In the plot above, we can see that the score variability is much higher when mothers are guardians, than when fathers and other are guardians. However, when we look at the frequency of each factor level:
```{r}
barplot(table(traindata$guardian), ylab = 'Number of participants')
```
We can see that this might just be because there is a lot more data for the mother category.

A small look at some other attributes:

```{r}
barplot(table(traindata$sex), ylab = 'Number of participants')
barplot(table(traindata$activities), ylab = 'Number of participants')
```
Frequency of the levels in these two attribute seems to indicate an even distribution. Which is good when looking for classification.


```{r}
x <- traindata %>% 
  correlate() %>% 
  focus(score)

x %>% 
  mutate(term = factor(term, levels = term[order(score)])) %>%  # Order by correlation strength
  ggplot(aes(x = term, y = score)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with score") +
    xlab("Variable")
```

```{r}
#| label: Principal component analysis

# Create df with only numeric variables
num_traindata <- traindata %>% 
  select(where(is.numeric))

PCAresults <- prcomp(num_traindata, scale = TRUE)
```

```{r}
#calculate total variance explained by each principal component
eigenvalues <- PCAresults$sdev^2
varexpl <- eigenvalues / sum(eigenvalues)
PCs <- 1:14

ggplot(data = NULL, aes(x = PCs, y = eigenvalues)) +
  geom_col() +
  scale_x_continuous(breaks = seq(1, 14, by = 1))

cumvar <- varexpl %>%
  cumsum() %>%
  as.data.frame() %>%
  mutate(component = 1:n())

colnames(cumvar) <- c("Cumulative Variance", "Component")

cumvar
```
We choose the first 6 principal components, because their eigenvalues are larger than 1.

```{r}

PCA_traindata <- traindata %>% 
  select(c(negate(is.numeric), score))

PCA_traindata <- cbind(PCA_traindata, PCAresults$x[,1:6])
```

# Model description

Briefly describe which models you compare to perform prediction. (approx. two or three paragraphs)

# Data transformation and pre-processing

Describe additional pre-processing steps you have used, if any (e.g., dealing with categorical data, scaling). If you do not do any pre-processing, you can leave this section out.

# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

# Team member contributions

Write down what each team member contributed to the project.

- DaniÃ«l: Data exploration, PCA, formatting the QMD file
- Max: Boosting & Random Forest analysis
- William: LASSO & Data exploration
- Madio: KNN & Data normalization