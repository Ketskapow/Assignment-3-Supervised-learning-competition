:::{.callout-note}
## Instruction

This is a quarto file. If you open its [source](report_template.qmd) in RStudio, you will get all the features of this interactive notebook-style document. If you want to know more about how to use this file to generate a .html document to hand in, look at [the getting started guide](https://quarto.org/docs/get-started/hello/rstudio.html).

(Delete this callout box before handing in your assignment.)
:::

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(corrr)
library(caret)
library(randomForest)
# additional packages here
```

```{r}
#| label: data loading
#| echo: false

testdata <- read_rds("data/test.rds")
traindata <- read_rds("data/train.rds")
```

# Data description

Before making any predictions, we want to look at the data and its structure

```{r}
#| label: data head

head(testdata)
```
We have 30 columns in this data, each row of this data likely representing a student and all their corresponding data.

We also eventually want to predict the score of a student with several attributes. We can already begin exploring the structure of some of these attributes:

```{r}
#| label: eda visualization

ggplot(traindata, aes(y = score, x = guardian)) +
  theme_minimal() +
  geom_boxplot()
```
In the plot above, we can see that the score variability is much higher when mothers are guardians, than when fathers and other are guardians. However, when we look at the frequency of each factor level:
```{r}
barplot(table(traindata$guardian), ylab = 'Number of participants')
```
We can see that this might just be because there is a lot more data for the mother category.

A small look at some other attributes:

```{r}
barplot(table(traindata$sex), ylab = 'Number of participants')
barplot(table(traindata$activities), ylab = 'Number of participants')
```
Frequency of the levels in these two attribute seems to indicate an even distribution. Which is good when looking for classification.


```{r}
x <- traindata %>% 
  correlate() %>% 
  focus(score)

x %>% 
  mutate(term = factor(term, levels = term[order(score)])) %>%  # Order by correlation strength
  ggplot(aes(x = term, y = score)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with score") +
    xlab("Variable")
```

# Model description

We look at the following models when making predictions:

## K-Nearest Neighbors (KNN)
KNN is a simple algorithm used for classification/regression. It operates by finding the K data points in the training set that are closest to the input data point, and then calculates the average of those points in the case of regression.

## Boosting
Boosting combines multiple weak learners (decision trees) to create a strong predictive model. It works by giving more weight to the data points that the weak models struggle with, making them focus on the mistakes.

## Random Forest
Random forest is a machine learning algorithm that creates a "forest" of decision trees. Each decision trees is trained on a random subset of the data with a random subset of features. For predictions, it aggregates the results from all the trees.

## LASSO regression
LASSO is a regression technique that focuses on feature selection. It adds a penalty term to the regression, which encourages the model to shrink less important coefficients to zero. A LASSO regression results in a simpler, more interpretable model that often performs well.


# Data transformation and pre-processing

## PCA

For preprocessing, we use PCA as a means of dimension reduction.

First, we create a dataframe with only the numeric variables, since PCA only works on numeric data.
```{r}
#| label: Principal component analysis

# Create df with only numeric variables
num_traindata <- traindata %>% 
  select(where(is.numeric), -score)
  
```

Then, we fit the PCA and extract the results using prcomp().
```{r}
PCAresults <- prcomp(num_traindata, scale = TRUE)
```

Now, we can calculate the eigenvalues of each principal component:
```{r}
#calculate total variance explained by each principal component
eigenvalues <- PCAresults$sdev^2
varexpl <- eigenvalues / sum(eigenvalues)
PCs <- 1:13

#plot components and their explained eigenvalues
ggplot(data = NULL, aes(x = PCs, y = eigenvalues)) +
  geom_col() +
  scale_x_continuous(breaks = seq(1, 13, by = 1))
```
If we use the Kaiser criterion, we use only principal components with an eigenvalue greater than 1. The principal components that fit this criterion are PC1 to PC5.

Let's see how much variance that explains:
```{r}
cumvar <- varexpl %>%
  cumsum() %>%
  as.data.frame() %>%
  mutate(component = 1:n())

colnames(cumvar) <- c("Cumulative Variance", "Component")

print(cumvar)
```
We will explain ~59% of the variance in the numeric data with the first 5 components, which is not bad.

Let's replace the numeric variables with these new principal components, except score, of course.
```{r}
PCA_traindata <- traindata %>% 
  select(c(negate(is.numeric), score))

PCA_traindata <- cbind(PCA_traindata, PCAresults$x[,1:5])
```

```{r}
#write.csv(PCA_traindata, "PCAdata")
```


Let's also check for missing data:
```{r}
sum(is.na(PCA_traindata))
```
There is none, so nothing needs to be done.

## RFE

Recursive feature elimination
```{r}
# Create a control function for feature selection
ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

dataminscore <- traindata %>% 
  select(-score)
# Perform feature selection with RFE
result <- rfe(dataminscore, traindata$score, sizes = c(1:10), rfeControl = ctrl)

print(result)
```

```{r}
RFEdata <- traindata %>% 
  select(result$optVariables)
```

# Model comparison

## KNN

### Building the model

### Hyperparameter tuning

### Final model & validation MSE

## Boosting

### Building the model

### Hyperparameter tuning

### Final model & validation MSE

## Random Forest

### Building the model

### Hyperparameter tuning

### Final model & validation MSE

## LASSO

### Building the model

### Hyperparameter tuning

### Final model & validation MSE

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

# Team member contributions

Write down what each team member contributed to the project.

- DaniÃ«l: Data exploration, PCA, formatting the QMD file
- Max: Boosting & Random Forest analysis
- William: LASSO & Data exploration
- Madio: KNN & Data normalization